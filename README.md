# Clip Embedding Reordering

CLIP embeddings generated by [clip-retrieval](https://github.com/rom1504/clip-retrieval) are not ordered the same as the webdataset they are generated from. This tool can reorder large CLIP embedding datasets such that they match the order of the image dataset they were generated from.

## Install

`git clone https://github.com/Veldrovive/embedding-dataset-reordering`

`cd embedding-dataset-reordering`

`pip install -e .`

## API

This module exposes three functions. Default values are meant to be evaluated from inside the `examples` folder.

For example, to download the test dataset, navigate to the root directory and run `cd examples && reorder-embeddings download-test-data`.

`reorder`: Takes as input an unordered embedding dataset along with metadata generated by `clip-retrieval` and reorders the embeddings to match the order of the image dataset.

**Note:** Before starting, you need to find the shard string width of your dataset. This is a manual task, but it is easy to find. Navigate to the metadata directory of your embedding dataset and run `reorder-embeddings get-example-key`.

This will print something similar to:
```
Example Keys:
Shard 3 has keys ['0000309', '0000321']
Shard 2 has keys ['0000209', '0000237']
Shard 0 has keys ['0000022', '0000031']
Shard 1 has keys ['0000114', '0000123']
```
By inspection, we can see that the first 5 characters represent the index of the shard (i.e. the keys for shard 3 start with 00003) so the shard width is 5.

Parameters
- `embeddings_folder`: Path to the folder containing the embedding `.npy` files.
- `metadata_folder`: Path to the folder containing the `.parquet` metadata files.
- `output_folder`: Path to the folder where the reordered `.npy` files will be saved.
- `shard_width`: The width of the shard string. Use the `get-example-key` function to find this.
- `start`: The starting index of the embedding dataset.
- `end`: The ending index of the embedding dataset.
- `intermediate_folder`: The path where intermediate files will be saved. For large datasets, these can become multiple terabytes so it is recommended to save these in a large distributed file system.
- `intermediate_partitions`: The number of partitions to break up the intermediate files into. This should be similar to the number of shards in your dataset.
- `parallelize_reading`: Whether to use spark to parallelize the download of the embeddings. This is only recommended if you have multiple executors available.
- `fill_missing`: Whether to fill in missing embeddings with zeros. This can occur if there were errors when downloading using img2dataset. Setting this to `True` will mean the filenames of the `.jpg`s will correspond to the index of the embeddings in the reordered dataset. Setting this to `False` means the position of the `.jpg`s will correspond to the index of the embeddings in the original dataset.
- `overwrite`: If `True`, overwrites existing files.

`download-test-data`: Uses img2dataset to download a test dataset. Run this from the `examples` directory to download the default one.

`test-inference`: Uses `clip-retrieval` to generate embeddings for the test dataset. Run this from the `examples` directory after downloading the test dataset.

## For development

Setup a virtualenv:

```
python3 -m venv .env
source .env/bin/activate
pip install -e .
```

to run tests:
```
pip install -r requirements-test.txt
```
then 
```
make lint
make test
```

You can use `make black` to reformat the code
